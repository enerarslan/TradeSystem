# AlphaTrade System - AI Agent Roadmap
## JPMorgan-Level Institutional Trading Platform

**Version:** 3.1 - AFML INSTITUTIONAL GRADE
**Status:** âœ… All Components Built + Full AFML Implementation
**Total Files:** 47+ Python files + configs + deployment
**Lines of Code:** ~25,000+
**Last Updated:** December 2024

---

## ğŸ¯ System Overview

A complete institutional-grade algorithmic trading system capable of:
- Live trading 46 US stocks using 15-min OHLCV data
- ML-based signal generation (XGBoost, LightGBM, CatBoost, Neural Networks)
- Professional risk management (VaR, position limits, circuit breakers)
- Algorithmic execution (TWAP, VWAP, POV, Adaptive)
- Real-time monitoring and reporting

### Version 3.1 AFML Institutional Features (NEW):
- **Information-Driven Bars** - Volume/Dollar/Tick bars for better statistical properties
- **Triple Barrier Method** - Path-dependent labeling with dynamic volatility barriers
- **Meta-Labeling Framework** - Two-stage approach separating direction from bet sizing
- **CUSUM Event Sampling** - Adaptive event detection for structural breaks
- **Sample Weight Calculation** - Handles overlapping labels with uniqueness weights
- **Clustered Feature Importance** - Hierarchical clustering for robust feature selection
- **Probabilistic Sharpe Ratio (PSR)** - Accounts for non-normality in returns
- **Deflated Sharpe Ratio (DSR)** - Adjusts for multiple testing / p-hacking
- **PurgedKFoldCV with 5% Embargo** - Eliminates serial correlation leakage
- **Feature Neutralization** - Market beta removal for alpha isolation
- **Winsorization Policy** - Preserves tail event information

### Version 3.0 Features (Previous):
- **Fractional Differentiation (FFD)** for stationary yet memory-preserving features
- **Hierarchical Risk Parity (HRP)** for robust portfolio optimization
- **Dynamic Transaction Cost Analysis** with Almgren-Chriss market impact
- **Numba JIT Acceleration** for 10-100x performance gains
- **Async Event-Driven Pipeline** for real-time trading
- **MLflow Experiment Tracking** for reproducible ML
- **DVC Data Versioning** for data pipeline management
- **SHAP Model Explainability** for regulatory compliance

---

## ğŸ› ï¸ Tech Stack (Implemented)

| Component | Technology |
|-----------|------------|
| Language | Python 3.11+ |
| Data Processing | pandas, numpy, scipy |
| ML/AI | XGBoost, LightGBM, CatBoost, PyTorch |
| Deep Learning | LSTM, Transformer, Attention |
| Technical Analysis | TA-Lib, pandas-ta |
| Broker API | Alpaca, Interactive Brokers |
| Database | PostgreSQL + TimescaleDB |
| Cache | Redis |
| Monitoring | Grafana, Prometheus |
| Deployment | Docker, docker-compose |
| **Performance** | **Numba JIT, asyncio, multiprocessing** |
| **MLOps** | **MLflow, DVC** |
| **Explainability** | **SHAP** |
| **Async** | **asyncio, aiohttp, websockets** |

---

## ğŸ“ Project Structure (Complete)

```
alphatrade/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml          âœ… Global configuration
â”‚   â”œâ”€â”€ symbols.yaml           âœ… 46-stock universe
â”‚   â””â”€â”€ risk_params.yaml       âœ… Risk parameters
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   âœ… 46 CSV files (15-min OHLCV)
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py          âœ… Multi-asset data loader
â”‚   â”‚   â”œâ”€â”€ preprocessor.py    âœ… Data cleaning + Information-Driven Bars (v3.1)
â”‚   â”‚   â”œâ”€â”€ labeling.py        âœ… NEW: Triple Barrier + Meta-Labeling (v3.1)
â”‚   â”‚   â”œâ”€â”€ database.py        âœ… PostgreSQL/TimescaleDB/Redis
â”‚   â”‚   â””â”€â”€ live_feed.py       âœ… WebSocket real-time feed
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ technical.py       âœ… 100+ technical indicators
â”‚   â”‚   â”œâ”€â”€ builder.py         âœ… Feature pipeline (200+ features)
â”‚   â”‚   â”œâ”€â”€ microstructure.py  âœ… Market microstructure
â”‚   â”‚   â”œâ”€â”€ cross_asset.py     âœ… Cross-asset analysis
â”‚   â”‚   â”œâ”€â”€ regime.py          âœ… HMM regime detection
â”‚   â”‚   â””â”€â”€ alternative.py     âœ… Alternative data
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py      âœ… Model registry & versioning
â”‚   â”‚   â”œâ”€â”€ ml_model.py        âœ… XGBoost/LightGBM/CatBoost/RF + MetaLabeling
â”‚   â”‚   â”œâ”€â”€ ensemble.py        âœ… Voting/Stacking/Blending
â”‚   â”‚   â”œâ”€â”€ deep_learning.py   âœ… LSTM/Transformer/Attention
â”‚   â”‚   â”œâ”€â”€ training.py        âœ… Walk-forward + PurgedKFoldCV 5% embargo + Clustered Feature Importance (v3.1)
â”‚   â”‚   â””â”€â”€ explainability.py  âœ… SHAP-based model explanations
â”‚   â”œâ”€â”€ strategy/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_strategy.py   âœ… Strategy framework
â”‚   â”‚   â”œâ”€â”€ momentum.py        âœ… Momentum & trend following
â”‚   â”‚   â”œâ”€â”€ mean_reversion.py  âœ… Mean reversion & pairs
â”‚   â”‚   â””â”€â”€ ml_strategy.py     âœ… ML-based strategies
â”‚   â”œâ”€â”€ risk/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ position_sizer.py  âœ… Kelly/Volatility/Risk Parity
â”‚   â”‚   â”œâ”€â”€ risk_manager.py    âœ… VaR/CVaR/Circuit breakers
â”‚   â”‚   â””â”€â”€ portfolio.py       âœ… Portfolio optimization + HRP
â”‚   â”œâ”€â”€ execution/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ broker_api.py      âœ… Alpaca & IBKR integration
â”‚   â”‚   â”œâ”€â”€ order_manager.py   âœ… Order lifecycle management
â”‚   â”‚   â”œâ”€â”€ executor.py        âœ… TWAP/VWAP/POV/Adaptive
â”‚   â”‚   â””â”€â”€ async_pipeline.py  âœ… Async event-driven pipeline
â”‚   â”œâ”€â”€ backtest/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ engine.py          âœ… Event-driven + Dynamic TCA
â”‚   â”‚   â””â”€â”€ metrics.py         âœ… Performance attribution + PSR/DSR (v3.1)
â”‚   â”œâ”€â”€ mlops/                  âœ… NEW - MLOps Module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ experiment_tracking.py âœ… MLflow integration
â”‚   â”‚   â””â”€â”€ dvc_config.py      âœ… Data version control
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py          âœ… Institutional logging
â”‚       â”œâ”€â”€ helpers.py         âœ… Utility functions
â”‚       â””â”€â”€ numba_accelerators.py âœ… JIT-compiled functions
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_db.sql            âœ… Database schema
â”‚   â””â”€â”€ train_models.py        âœ… AFML Institutional Training Pipeline (v3.1)
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml     âœ… Metrics config
â”œâ”€â”€ models/                    ğŸ“ Trained models (generated)
â”œâ”€â”€ results/                   ğŸ“ Backtest results (generated)
â”œâ”€â”€ logs/                      ğŸ“ Log files (generated)
â”œâ”€â”€ notebooks/                 ğŸ“ Research notebooks
â”œâ”€â”€ main.py                    âœ… Main orchestrator
â”œâ”€â”€ Dockerfile                 âœ… Production container
â”œâ”€â”€ Dockerfile.jupyter         âœ… Research environment
â”œâ”€â”€ docker-compose.yaml        âœ… Full stack deployment
â”œâ”€â”€ requirements.txt           âœ… Production dependencies
â”œâ”€â”€ requirements-research.txt  âœ… Research dependencies
â”œâ”€â”€ setup.py                   âœ… Package setup
â”œâ”€â”€ .env.example              âœ… Environment template
â””â”€â”€ .gitignore                âœ… Git ignore rules
```

---

## ğŸš€ QUICK START GUIDE

### Step 1: Environment Setup

```bash
# 1. Create virtual environment
python -m venv venv

# 2. Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt
```

### Step 2: Configure Environment

```bash
# 1. Copy environment template
cp .env.example .env

# 2. Edit .env file with your credentials:
#    - ALPACA_API_KEY
#    - ALPACA_API_SECRET
#    - POSTGRES_PASSWORD
```

### Step 3: Verify Data

```bash
# Check that CSV files are in data/raw/
ls data/raw/
# Should show: AAPL_15min.csv, MSFT_15min.csv, etc.
```

### Step 4: Run Backtest (First Test)

```bash
python main.py --mode backtest
```

### Step 5: Train ML Models (Optional)

```bash
python scripts/train_models.py
```

### Step 6: Run Paper Trading

```bash
python main.py --mode paper
```

### Step 7: Docker Deployment (Production)

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f alphatrade

# Stop services
docker-compose down
```

---

## ğŸ“‹ EXECUTION ORDER FOR AI AGENT

### Phase 1: Initial Setup & Verification
```
Order | File/Command | Purpose
------|--------------|--------
1     | pip install -r requirements.txt | Install all dependencies
2     | Verify data/raw/*.csv exists | Check 46 CSV files present
3     | python -c "from src.utils.logger import get_logger" | Test imports
```

### Phase 2: Data Pipeline Test
```
Order | File/Command | Purpose
------|--------------|--------
4     | python -c "from src.data.loader import DataLoader; dl = DataLoader(); print(dl.load('AAPL').head())" | Test data loading
5     | python -c "from src.data.preprocessor import DataPreprocessor; dp = DataPreprocessor()" | Test preprocessor
```

### Phase 3: Feature Engineering Test
```
Order | File/Command | Purpose
------|--------------|--------
6     | python -c "from src.features.technical import TechnicalIndicators; ti = TechnicalIndicators()" | Test indicators
7     | python -c "from src.features.builder import FeatureBuilder; fb = FeatureBuilder()" | Test feature builder
```

### Phase 4: Run Backtest
```
Order | File/Command | Purpose
------|--------------|--------
8     | python main.py --mode backtest | Full backtest run
```

### Phase 5: Train Models (Optional but Recommended)
```
Order | File/Command | Purpose
------|--------------|--------
9     | python scripts/train_models.py | Train XGBoost, LightGBM, CatBoost
10    | Check models/ directory | Verify model files created
```

### Phase 6: Paper Trading
```
Order | File/Command | Purpose
------|--------------|--------
11    | Set ALPACA_API_KEY in .env | Configure broker
12    | Set ALPACA_API_SECRET in .env | Configure broker
13    | python main.py --mode paper | Start paper trading
```

### Phase 7: Production Deployment (Docker)
```
Order | File/Command | Purpose
------|--------------|--------
14    | docker-compose up -d postgres redis | Start databases
15    | docker-compose up -d alphatrade | Start trading system
16    | docker-compose up -d grafana | Start monitoring
17    | Access http://localhost:3000 | View Grafana dashboard
```

---

## ğŸ”§ COMPONENT DETAILS

### Data Layer (`src/data/`)

| File | Features |
|------|----------|
| `loader.py` | Multi-asset parallel loading, CSV/Parquet/API support |
| `preprocessor.py` | Gap filling, Winsorization (not drop), **Information-Driven Bars (Volume/Dollar/Tick)** |
| `labeling.py` | **NEW v3.1:** Triple Barrier Method, Meta-Labeling, CUSUM Filter, Sample Weights |
| `database.py` | TimescaleDB hypertables, Redis caching |
| `live_feed.py` | Alpaca/Polygon WebSocket, bar aggregation |

### Feature Engineering (`src/features/`)

| File | Features |
|------|----------|
| `technical.py` | 100+ indicators: SMA, EMA, RSI, MACD, Bollinger, Ichimoku, ATR, etc. |
| `builder.py` | 200+ total features, automatic feature selection, **Triple Barrier Method**, **Fractional Differentiation** |
| `microstructure.py` | Kyle's Lambda, VPIN, Amihud illiquidity, Roll spread, **Level 2 Order Book Features** |
| `cross_asset.py` | Rolling correlations, beta, sector momentum |
| `regime.py` | HMM-based regime detection (bull/bear/sideways) |
| `alternative.py` | Sentiment, economic indicators, options-derived |

### ML Models (`src/models/`)

| File | Features |
|------|----------|
| `ml_model.py` | XGBoost, LightGBM, CatBoost, RandomForest with GPU, **MetaLabelingModel** |
| `ensemble.py` | VotingEnsemble, StackingEnsemble, BlendingEnsemble |
| `deep_learning.py` | Bidirectional LSTM, Transformer with attention |
| `training.py` | Walk-forward, Optuna, **PurgedKFoldCV (5% embargo)**, **Combinatorial Purged CV**, **Clustered Feature Importance (MDI/MDA)** |
| `explainability.py` | **SHAP-based explanations**, feature importance, waterfall/force plots |

### Strategy Framework (`src/strategy/`)

| File | Features |
|------|----------|
| `momentum.py` | Multi-timeframe momentum, breakout detection |
| `mean_reversion.py` | Z-score mean reversion, pairs trading with cointegration |
| `ml_strategy.py` | ML signal generation, confidence thresholds |

### Risk Management (`src/risk/`)

| File | Features |
|------|----------|
| `position_sizer.py` | Kelly Criterion, Volatility-based, Risk Parity, Optimal-F |
| `risk_manager.py` | VaR (95%, 99%), CVaR, circuit breakers, pre-trade checks |
| `portfolio.py` | MVO, Black-Litterman, Maximum Diversification, **Hierarchical Risk Parity (HRP)** |

### Execution (`src/execution/`)

| File | Features |
|------|----------|
| `broker_api.py` | Alpaca REST + WebSocket, IBKR TWS API |
| `order_manager.py` | Order lifecycle, smart order routing |
| `executor.py` | TWAP, VWAP, POV, Adaptive execution algorithms |
| `async_pipeline.py` | **Async event-driven pipeline**, priority queues, parallel workers |

### Backtesting (`src/backtest/`)

| File | Features |
|------|----------|
| `engine.py` | Event-driven + vectorized, realistic fills, slippage, **Dynamic Transaction Cost Analysis (Almgren-Chriss)** |
| `metrics.py` | Sharpe, Sortino, Calmar, Max DD, attribution, **Probabilistic SR (PSR)**, **Deflated SR (DSR)**, **Minimum Track Record Length** |

### MLOps (`src/mlops/`) - NEW

| File | Features |
|------|----------|
| `experiment_tracking.py` | **MLflow integration**, experiment management, model registry, artifact logging |
| `dvc_config.py` | **DVC data versioning**, pipeline management, remote storage, data lineage |

### Performance Utils (`src/utils/`)

| File | Features |
|------|----------|
| `numba_accelerators.py` | **Numba JIT-compiled** indicators (EMA, RSI, ATR, MACD), FFD, triple barrier, rolling stats |
| `logger.py` | Institutional-grade logging with rotation |
| `helpers.py` | Utility functions |

---

## âš ï¸ IMPORTANT NOTES

### Before Running Live:
1. âœ… Test thoroughly with backtest mode
2. âœ… Run paper trading for at least 1 week
3. âœ… Verify all risk limits are correctly set
4. âœ… Check broker API credentials
5. âœ… Monitor logs for errors

### Risk Defaults (config/risk_params.yaml):
- Max position: 10% of portfolio
- Max sector: 30% of portfolio
- Max drawdown: 15%
- Daily loss limit: 3%
- Circuit breaker: 3% intraday loss

### Required API Keys:
- Alpaca API Key & Secret (for paper/live trading)
- Optional: Polygon API Key (for additional data)

---

## ğŸ“Š Monitoring URLs (After Docker Deploy)

| Service | URL | Default Credentials |
|---------|-----|---------------------|
| Grafana | http://localhost:3000 | admin / admin123 |
| Prometheus | http://localhost:9090 | - |
| Jupyter | http://localhost:8888 | Token in .env |
| PostgreSQL | localhost:5432 | trading / (see .env) |
| Redis | localhost:6379 | - |

---

## ğŸ“ Next Steps for Enhancement

1. **Add More Strategies**: Implement sector rotation, factor investing
2. **Enhance ML**: Add reinforcement learning, online learning
3. **Options Trading**: Extend to options strategies
4. **Multi-Asset**: Add crypto, forex support
5. **Cloud Deployment**: AWS/GCP with auto-scaling

---

## ğŸ“ˆ VERSION 3.0 IMPLEMENTATION DETAILS

### Phase 1: Advanced Data Science (COMPLETED âœ…)

| Component | Status | Description |
|-----------|--------|-------------|
| Fractional Differentiation | âœ… | Fixed-window FFD with ADF test for stationarity |
| Level 2 Microstructure | âœ… | Order book imbalance, depth, spread analysis |
| Feature Neutralization | âœ… | Cross-sectional neutralization, sector-relative features |
| Robust Outlier Handling | âœ… | Winsorization, MAD-based detection |

### Phase 2: Institutional Labeling (COMPLETED âœ…)

| Component | Status | Description |
|-----------|--------|-------------|
| Triple Barrier Method | âœ… | Volatility-based barriers, asymmetric targets (AFML Ch.3) |
| Meta-Labeling Framework | âœ… | Secondary model filtering, Kelly criterion bet sizing |
| Purged K-Fold CV | âœ… | Embargo periods, combinatorial CV, group-aware purging |

### Phase 3: Portfolio & Risk Management (COMPLETED âœ…)

| Component | Status | Description |
|-----------|--------|-------------|
| Hierarchical Risk Parity | âœ… | Quasi-diagonalization, recursive bisection, rolling HRP |
| Dynamic Transaction Costs | âœ… | Almgren-Chriss market impact, optimal execution scheduling |

### Phase 4: Infrastructure & Performance (COMPLETED âœ…)

| Component | Status | Description |
|-----------|--------|-------------|
| Numba JIT Acceleration | âœ… | 10-100x speedup for indicators, FFD, triple barrier |
| Async Trading Pipeline | âœ… | Event-driven, priority queues, parallel feature computation |

### Phase 5: MLOps & Explainability (COMPLETED âœ…)

| Component | Status | Description |
|-----------|--------|-------------|
| MLflow Integration | âœ… | Experiment tracking, model registry, artifact management |
| DVC Data Versioning | âœ… | Data pipelines, remote storage, reproducibility |
| SHAP Explainability | âœ… | Feature importance, waterfall plots, regime explanations |

### Phase 6: AFML Institutional Methodology v3.1 (COMPLETED âœ…)

| Component | Status | Description |
|-----------|--------|-------------|
| Information-Driven Bars | âœ… | Volume/Dollar/Tick bars - better statistical properties than time bars |
| Triple Barrier Method | âœ… | Path-dependent labeling with dynamic volatility barriers (AFML Ch.3) |
| Meta-Labeling | âœ… | Two-stage approach: primary model (direction) + secondary model (bet sizing) |
| CUSUM Event Filter | âœ… | Adaptive event sampling for structural break detection |
| Sample Weight Calculation | âœ… | Uniqueness weights + time decay for overlapping labels |
| Clustered Feature Importance | âœ… | Hierarchical clustering + MDI/MDA at cluster level (AFML Ch.8) |
| Probabilistic Sharpe Ratio | âœ… | Accounts for skewness/kurtosis in returns distribution |
| Deflated Sharpe Ratio | âœ… | Adjusts for multiple testing / p-hacking bias |
| PurgedKFoldCV 5% Embargo | âœ… | Minimum 5% embargo to eliminate serial correlation leakage |
| Feature Neutralization | âœ… | Market beta removal, microstructure feature downweighting |
| Winsorization Policy | âœ… | Default outlier handling preserves tail event information |

---

## ğŸ”¬ ADVANCED FEATURES USAGE

### Triple Barrier Method (v3.1)
```python
from src.data.labeling import TripleBarrierLabeler, TripleBarrierConfig

config = TripleBarrierConfig(
    pt_sl_ratio=(1.0, 1.0),      # Symmetric barriers
    volatility_lookback=20,      # EWM volatility window
    max_holding_period=10,       # Max bars to hold
    min_return=0.001             # Minimum return threshold
)

labeler = TripleBarrierLabeler(config)
events = labeler.get_events_with_ohlcv(
    prices=df,                   # OHLCV DataFrame
    pt_sl=(1.0, 1.0)            # Profit/StopLoss multipliers
)
# events contains: label, bin_label, t1, ret, touch_type
```

### Meta-Labeling with Bet Sizing (v3.1)
```python
from src.data.labeling import MetaLabeler, MetaLabelingConfig

config = MetaLabelingConfig(
    primary_threshold=0.5,
    use_probability=True
)
meta_labeler = MetaLabeler(config)

# Get side from primary model
side = meta_labeler.get_primary_side(primary_predictions)

# Create meta-labels
meta_events = meta_labeler.create_meta_labels(triple_barrier_events, side)

# Prepare training data for secondary model
X_meta, y_meta = meta_labeler.get_meta_training_data(features, meta_events)
```

### Information-Driven Bars (v3.1)
```python
from src.data.preprocessor import convert_time_bars_to_information_bars

# Convert 15-min bars to dollar bars
dollar_bars = convert_time_bars_to_information_bars(
    time_bars=df,
    bar_type="dollar",           # "volume", "dollar", or "tick"
    target_bars_per_day=50       # Auto-estimate threshold
)
# Returns IID-normal distributed returns with lower serial correlation
```

### Purged Cross-Validation with 5% Embargo (v3.1)
```python
from src.models.training import CrossValidationTrainer

cv_trainer = CrossValidationTrainer(
    cv_method='purged_kfold',
    n_splits=5,
    purge_gap=0,
    embargo_pct=0.05             # Minimum 5% per AFML recommendations
)
results = cv_trainer.cross_validate(model, X, y, t1=events['t1'])
```

### Clustered Feature Importance (v3.1)
```python
from src.models.training import feature_importance_with_clustering

result = feature_importance_with_clustering(
    model=fitted_model,
    X=features,
    y=labels,
    n_clusters=None,             # Auto-determine via silhouette
    method='mda',                # 'mda' or 'mdi'
    n_iterations=10
)
# Returns: cluster_importance, feature_importance, clusters, selected_features
```

### Probabilistic & Deflated Sharpe Ratio (v3.1)
```python
from src.backtest.metrics import SharpeRatioStatistics

sr_stats = SharpeRatioStatistics(periods_per_year=252)
report = sr_stats.generate_sharpe_report(
    returns=strategy_returns,
    n_trials=10,                 # Number of backtests run
    sr_benchmark=0.0,
    confidence=0.95
)
# report contains: sharpe_ratio, probabilistic_sr, deflated_sr,
# minimum_track_record, confidence_interval, interpretation
```

### Hierarchical Risk Parity
```python
from src.risk.portfolio import HierarchicalRiskParity

hrp = HierarchicalRiskParity()
weights = hrp.optimize(returns_df)
# Or rolling optimization
rolling_weights = hrp.rolling_optimize(returns_df, window=252)
```

### Async Trading Pipeline
```python
from src.execution.async_pipeline import PipelineBuilder

pipeline = (PipelineBuilder()
    .with_data_source('alpaca', symbols=['AAPL', 'MSFT'])
    .with_feature_builder(feature_builder)
    .with_model(ml_model)
    .with_risk_manager(risk_manager)
    .with_broker(alpaca_broker)
    .build())

await pipeline.start()
```

### MLflow Experiment Tracking
```python
from src.mlops.experiment_tracking import MLflowTracker

tracker = MLflowTracker(experiment_name='strategy_v3')
with tracker.start_run(run_name='lgbm_triple_barrier'):
    tracker.log_params(model_params)
    tracker.log_metrics(backtest_results)
    tracker.log_model(model, 'lightgbm')
```

### SHAP Explainability
```python
from src.models.explainability import TradingExplainer

explainer = TradingExplainer(model, X_train, feature_names)
explainer.generate_report(X_test, output_dir='reports/shap')
regime_analysis = explainer.explain_by_regime(X_test, regimes)
```

---

*Document Version: 3.1*
*Implementation Status: COMPLETE + FULL AFML INSTITUTIONAL METHODOLOGY*
*Ready for: Backtest â†’ Paper Trading â†’ Live Trading*
*AFML Features: Triple Barrier, Meta-Labeling, Information Bars, Clustered Importance, PSR/DSR, PurgedKFoldCV 5% Embargo*

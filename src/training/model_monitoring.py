"""
Model Monitoring Module - Production ML Model Health Tracking.

JPMorgan Institutional-Level Model Performance Monitoring.

This module provides real-time monitoring of deployed ML models including:
- Prediction drift detection
- Feature drift detection
- Performance degradation alerts
- Automated retraining triggers
- Comprehensive reporting

Reference:
    "Machine Learning Design Patterns" by Lakshmanan et al.
    "Reliable Machine Learning" by Murphy & Talwalkar
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd

try:
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False


logger = logging.getLogger(__name__)


class AlertLevel(str, Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


class DriftType(str, Enum):
    """Types of drift to monitor."""
    PREDICTION = "prediction"
    FEATURE = "feature"
    PERFORMANCE = "performance"
    CONCEPT = "concept"


@dataclass
class DriftResult:
    """Result of drift detection."""
    drift_type: DriftType
    drift_detected: bool
    drift_score: float
    p_value: float
    threshold: float
    details: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class MonitoringAlert:
    """Alert generated by monitoring system."""
    level: AlertLevel
    message: str
    metric_name: str
    current_value: float
    threshold: float
    timestamp: datetime = field(default_factory=datetime.now)
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ModelHealthReport:
    """Comprehensive model health report."""
    model_id: str
    report_time: datetime
    health_score: float  # 0-100
    prediction_drift: Optional[DriftResult]
    feature_drift: Dict[str, DriftResult]
    performance_metrics: Dict[str, float]
    alerts: List[MonitoringAlert]
    recommendations: List[str]


class PredictionDriftDetector:
    """
    Detect drift in model predictions over time.

    Uses statistical tests to compare recent predictions
    against a baseline distribution.
    """

    def __init__(
        self,
        baseline_window: int = 1000,
        test_window: int = 100,
        significance_level: float = 0.05,
    ):
        """
        Initialize prediction drift detector.

        Args:
            baseline_window: Number of predictions for baseline
            test_window: Number of recent predictions to test
            significance_level: P-value threshold for drift detection
        """
        self.baseline_window = baseline_window
        self.test_window = test_window
        self.significance_level = significance_level

        self._baseline_predictions: List[float] = []
        self._recent_predictions: List[float] = []
        self._baseline_set = False

    def update(self, prediction: float) -> None:
        """Add new prediction to the buffer."""
        if not self._baseline_set:
            self._baseline_predictions.append(prediction)
            if len(self._baseline_predictions) >= self.baseline_window:
                self._baseline_set = True
        else:
            self._recent_predictions.append(prediction)
            # Keep buffer at test_window size
            if len(self._recent_predictions) > self.test_window:
                self._recent_predictions.pop(0)

    def check_drift(self) -> DriftResult:
        """Check for prediction drift using KS test."""
        if not self._baseline_set or len(self._recent_predictions) < self.test_window // 2:
            return DriftResult(
                drift_type=DriftType.PREDICTION,
                drift_detected=False,
                drift_score=0.0,
                p_value=1.0,
                threshold=self.significance_level,
                details={"status": "insufficient_data"}
            )

        if not SCIPY_AVAILABLE:
            logger.warning("scipy not available for drift detection")
            return DriftResult(
                drift_type=DriftType.PREDICTION,
                drift_detected=False,
                drift_score=0.0,
                p_value=1.0,
                threshold=self.significance_level,
            )

        # Kolmogorov-Smirnov test
        statistic, p_value = stats.ks_2samp(
            self._baseline_predictions,
            self._recent_predictions
        )

        drift_detected = p_value < self.significance_level

        return DriftResult(
            drift_type=DriftType.PREDICTION,
            drift_detected=drift_detected,
            drift_score=statistic,
            p_value=p_value,
            threshold=self.significance_level,
            details={
                "baseline_mean": np.mean(self._baseline_predictions),
                "recent_mean": np.mean(self._recent_predictions),
                "baseline_std": np.std(self._baseline_predictions),
                "recent_std": np.std(self._recent_predictions),
            }
        )

    def reset_baseline(self) -> None:
        """Reset baseline with current recent predictions."""
        if self._recent_predictions:
            self._baseline_predictions = self._recent_predictions.copy()
            self._recent_predictions = []
            self._baseline_set = True
            logger.info("Prediction baseline reset")


class FeatureDriftDetector:
    """
    Detect drift in input feature distributions.

    Monitors each feature independently and flags
    significant distribution changes.
    """

    def __init__(
        self,
        feature_names: List[str],
        baseline_window: int = 1000,
        test_window: int = 100,
        significance_level: float = 0.05,
    ):
        """
        Initialize feature drift detector.

        Args:
            feature_names: List of feature names to monitor
            baseline_window: Number of samples for baseline
            test_window: Number of recent samples to test
            significance_level: P-value threshold
        """
        self.feature_names = feature_names
        self.baseline_window = baseline_window
        self.test_window = test_window
        self.significance_level = significance_level

        self._baseline_data: Dict[str, List[float]] = {f: [] for f in feature_names}
        self._recent_data: Dict[str, List[float]] = {f: [] for f in feature_names}
        self._baseline_set = False

    def update(self, features: Dict[str, float]) -> None:
        """Add new feature values to the buffer."""
        for name, value in features.items():
            if name not in self.feature_names:
                continue

            if not self._baseline_set:
                self._baseline_data[name].append(value)
            else:
                self._recent_data[name].append(value)
                if len(self._recent_data[name]) > self.test_window:
                    self._recent_data[name].pop(0)

        # Check if baseline is complete
        if not self._baseline_set:
            min_samples = min(len(v) for v in self._baseline_data.values())
            if min_samples >= self.baseline_window:
                self._baseline_set = True

    def check_drift(self) -> Dict[str, DriftResult]:
        """Check for drift in each feature."""
        results = {}

        if not self._baseline_set:
            for name in self.feature_names:
                results[name] = DriftResult(
                    drift_type=DriftType.FEATURE,
                    drift_detected=False,
                    drift_score=0.0,
                    p_value=1.0,
                    threshold=self.significance_level,
                    details={"status": "baseline_not_set"}
                )
            return results

        for name in self.feature_names:
            baseline = self._baseline_data.get(name, [])
            recent = self._recent_data.get(name, [])

            if len(recent) < self.test_window // 2:
                results[name] = DriftResult(
                    drift_type=DriftType.FEATURE,
                    drift_detected=False,
                    drift_score=0.0,
                    p_value=1.0,
                    threshold=self.significance_level,
                    details={"status": "insufficient_data"}
                )
                continue

            if SCIPY_AVAILABLE:
                statistic, p_value = stats.ks_2samp(baseline, recent)
            else:
                statistic, p_value = 0.0, 1.0

            results[name] = DriftResult(
                drift_type=DriftType.FEATURE,
                drift_detected=p_value < self.significance_level,
                drift_score=statistic,
                p_value=p_value,
                threshold=self.significance_level,
                details={
                    "feature_name": name,
                    "baseline_mean": np.mean(baseline) if baseline else 0,
                    "recent_mean": np.mean(recent) if recent else 0,
                }
            )

        return results

    def get_drifting_features(self) -> List[str]:
        """Get list of features currently showing drift."""
        results = self.check_drift()
        return [name for name, result in results.items() if result.drift_detected]


class PerformanceMonitor:
    """
    Monitor model performance metrics over time.

    Tracks key metrics and alerts when performance
    degrades below thresholds.
    """

    def __init__(
        self,
        metrics: List[str] = None,
        window_size: int = 100,
        degradation_threshold: float = 0.1,
    ):
        """
        Initialize performance monitor.

        Args:
            metrics: List of metric names to track
            window_size: Rolling window for metric calculation
            degradation_threshold: Fractional degradation that triggers alert
        """
        self.metrics = metrics or ["accuracy", "precision", "recall", "f1", "sharpe"]
        self.window_size = window_size
        self.degradation_threshold = degradation_threshold

        self._metric_history: Dict[str, List[float]] = {m: [] for m in self.metrics}
        self._baseline_metrics: Dict[str, float] = {}

    def set_baseline(self, metrics: Dict[str, float]) -> None:
        """Set baseline performance metrics."""
        self._baseline_metrics = metrics.copy()
        logger.info(f"Performance baseline set: {metrics}")

    def update(self, metrics: Dict[str, float]) -> List[MonitoringAlert]:
        """
        Update with new metric values and check for degradation.

        Args:
            metrics: Dictionary of metric name -> value

        Returns:
            List of alerts for degraded metrics
        """
        alerts = []

        for name, value in metrics.items():
            if name not in self.metrics:
                continue

            self._metric_history[name].append(value)
            if len(self._metric_history[name]) > self.window_size:
                self._metric_history[name].pop(0)

            # Check for degradation
            if name in self._baseline_metrics:
                baseline = self._baseline_metrics[name]
                if baseline > 0:
                    degradation = (baseline - value) / baseline
                    if degradation > self.degradation_threshold:
                        alerts.append(MonitoringAlert(
                            level=AlertLevel.WARNING if degradation < 0.2 else AlertLevel.CRITICAL,
                            message=f"Performance degradation in {name}: {degradation*100:.1f}% below baseline",
                            metric_name=name,
                            current_value=value,
                            threshold=baseline * (1 - self.degradation_threshold),
                            details={
                                "baseline": baseline,
                                "degradation_pct": degradation * 100,
                            }
                        ))

        return alerts

    def get_rolling_metrics(self) -> Dict[str, float]:
        """Get rolling average of each metric."""
        return {
            name: np.mean(values) if values else 0.0
            for name, values in self._metric_history.items()
        }


class ModelMonitor:
    """
    Comprehensive model monitoring system.

    Combines prediction drift, feature drift, and performance
    monitoring into a unified interface.

    Example:
        monitor = ModelMonitor(
            model_id="my_model_v1",
            feature_names=["feature_1", "feature_2"],
        )

        # During inference
        for X_batch, y_true in data_stream:
            predictions = model.predict(X_batch)

            # Update monitor
            alerts = monitor.update(
                predictions=predictions,
                features=X_batch,
                actuals=y_true,
            )

            for alert in alerts:
                if alert.level == AlertLevel.CRITICAL:
                    trigger_retraining()
    """

    def __init__(
        self,
        model_id: str,
        feature_names: List[str],
        baseline_window: int = 1000,
        test_window: int = 100,
        performance_metrics: List[str] = None,
    ):
        """
        Initialize the model monitor.

        Args:
            model_id: Unique identifier for the model
            feature_names: List of feature names
            baseline_window: Baseline sample size
            test_window: Test window size
            performance_metrics: Metrics to track
        """
        self.model_id = model_id
        self.feature_names = feature_names

        # Initialize sub-monitors
        self.prediction_drift = PredictionDriftDetector(
            baseline_window=baseline_window,
            test_window=test_window,
        )
        self.feature_drift = FeatureDriftDetector(
            feature_names=feature_names,
            baseline_window=baseline_window,
            test_window=test_window,
        )
        self.performance_monitor = PerformanceMonitor(
            metrics=performance_metrics,
            window_size=test_window,
        )

        self._alerts_history: List[MonitoringAlert] = []
        self._last_report_time: datetime = datetime.now()

    def update(
        self,
        predictions: np.ndarray,
        features: Union[np.ndarray, pd.DataFrame],
        actuals: Optional[np.ndarray] = None,
        metrics: Optional[Dict[str, float]] = None,
    ) -> List[MonitoringAlert]:
        """
        Update monitor with new predictions and features.

        Args:
            predictions: Model predictions
            features: Input features
            actuals: Actual values (if available)
            metrics: Performance metrics (if computed externally)

        Returns:
            List of alerts generated
        """
        alerts = []

        # Update prediction drift detector
        for pred in np.atleast_1d(predictions):
            self.prediction_drift.update(float(pred))

        # Update feature drift detector
        if isinstance(features, pd.DataFrame):
            for idx in range(len(features)):
                row_dict = features.iloc[idx].to_dict()
                self.feature_drift.update(row_dict)
        else:
            for row in features:
                row_dict = {self.feature_names[i]: row[i] for i in range(len(row))}
                self.feature_drift.update(row_dict)

        # Check prediction drift
        pred_drift = self.prediction_drift.check_drift()
        if pred_drift.drift_detected:
            alerts.append(MonitoringAlert(
                level=AlertLevel.WARNING,
                message=f"Prediction distribution drift detected (p={pred_drift.p_value:.4f})",
                metric_name="prediction_drift",
                current_value=pred_drift.drift_score,
                threshold=pred_drift.threshold,
                details=pred_drift.details,
            ))

        # Check feature drift
        feature_drifts = self.feature_drift.check_drift()
        drifting_features = [name for name, result in feature_drifts.items() if result.drift_detected]
        if drifting_features:
            alerts.append(MonitoringAlert(
                level=AlertLevel.WARNING if len(drifting_features) < 5 else AlertLevel.CRITICAL,
                message=f"Feature drift detected in {len(drifting_features)} features: {drifting_features[:5]}",
                metric_name="feature_drift",
                current_value=len(drifting_features),
                threshold=5,
                details={"drifting_features": drifting_features},
            ))

        # Update performance monitor if metrics provided
        if metrics:
            perf_alerts = self.performance_monitor.update(metrics)
            alerts.extend(perf_alerts)

        # Store alerts
        self._alerts_history.extend(alerts)

        return alerts

    def generate_report(self) -> ModelHealthReport:
        """Generate comprehensive health report."""
        pred_drift = self.prediction_drift.check_drift()
        feature_drifts = self.feature_drift.check_drift()

        # Calculate health score (0-100)
        health_score = 100.0

        # Deduct for prediction drift
        if pred_drift.drift_detected:
            health_score -= 20

        # Deduct for feature drift
        n_drifting = sum(1 for r in feature_drifts.values() if r.drift_detected)
        health_score -= min(30, n_drifting * 5)

        # Deduct for recent critical alerts
        recent_critical = sum(
            1 for a in self._alerts_history[-50:]
            if a.level == AlertLevel.CRITICAL
        )
        health_score -= min(30, recent_critical * 10)

        health_score = max(0, health_score)

        # Generate recommendations
        recommendations = []
        if pred_drift.drift_detected:
            recommendations.append("Consider retraining model - prediction distribution has shifted")
        if n_drifting > 0:
            recommendations.append(f"Investigate {n_drifting} drifting features")
        if health_score < 50:
            recommendations.append("URGENT: Model health critical, immediate attention required")

        return ModelHealthReport(
            model_id=self.model_id,
            report_time=datetime.now(),
            health_score=health_score,
            prediction_drift=pred_drift,
            feature_drift=feature_drifts,
            performance_metrics=self.performance_monitor.get_rolling_metrics(),
            alerts=self._alerts_history[-20:],  # Last 20 alerts
            recommendations=recommendations,
        )

    def should_retrain(self, health_threshold: float = 50.0) -> bool:
        """Check if model should be retrained based on health score."""
        report = self.generate_report()
        return report.health_score < health_threshold

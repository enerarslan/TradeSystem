"""
KURUMSAL SEVÄ°YE CSV VERÄ° YÃœKLEYICI
JPMorgan Quant Research tarzÄ± veri yÃ¶netimi

Ã–zellikler:
- Ã‡oklu dosya formatÄ± desteÄŸi (CSV, XLSX)
- Veri doÄŸrulama ve temizleme
- Otomatik tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- Eksik veri interpolasyonu
- Performans optimizasyonu (chunk processing)
- DetaylÄ± hata raporlama
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Optional, Dict, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

from data.models import MarketTick
from utils.logger import log


@dataclass
class DataQualityReport:
    """Veri kalitesi raporu"""
    symbol: str
    total_rows: int
    missing_values: Dict[str, int]
    duplicates: int
    date_range: Tuple[datetime, datetime]
    gaps_detected: int
    anomalies: int
    quality_score: float  # 0-100


class LocalCSVLoader:
    """
    Profesyonel CSV/Excel veri yÃ¼kleyici.
    
    Desteklenen formatlar:
    - CSV: {symbol}_15min.csv
    - Excel: {symbol}_15min.xlsx
    
    Beklenen kolonlar:
    - timestamp: Zaman damgasÄ± (YYYY-MM-DD HH:MM:SS)
    - open: AÃ§Ä±lÄ±ÅŸ fiyatÄ±
    - high: En yÃ¼ksek fiyat
    - low: En dÃ¼ÅŸÃ¼k fiyat
    - close: KapanÄ±ÅŸ fiyatÄ±
    - volume: Ä°ÅŸlem hacmi
    """
    
    def __init__(
        self, 
        storage_path: str = "data/storage",
        validate_data: bool = True,
        interpolate_missing: bool = True,
        remove_outliers: bool = True
    ):
        """
        Args:
            storage_path: CSV dosyalarÄ±nÄ±n bulunduÄŸu klasÃ¶r
            validate_data: Veri doÄŸrulama yapÄ±lsÄ±n mÄ±?
            interpolate_missing: Eksik deÄŸerleri interpolate et
            remove_outliers: AykÄ±rÄ± deÄŸerleri temizle
        """
        self.storage_path = Path(storage_path)
        self.validate_data = validate_data
        self.interpolate_missing = interpolate_missing
        self.remove_outliers = remove_outliers
        
        # Cache (AynÄ± sembolÃ¼ tekrar yÃ¼klemekten kaÃ§Ä±n)
        self._cache: Dict[str, List[MarketTick]] = {}
        
        # Veri kalite raporlarÄ±
        self.quality_reports: Dict[str, DataQualityReport] = {}
        
        # Ä°statistikler
        self.stats = {
            'files_loaded': 0,
            'total_rows': 0,
            'cache_hits': 0,
            'errors': 0
        }
        
        if not self.storage_path.exists():
            log.warning(f"âš ï¸ Storage klasÃ¶rÃ¼ bulunamadÄ±: {self.storage_path}")
            log.info(f"ğŸ“ KlasÃ¶r oluÅŸturuluyor: {self.storage_path}")
            self.storage_path.mkdir(parents=True, exist_ok=True)
    
    def load_data(
        self, 
        symbol: str,
        use_cache: bool = True,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> List[MarketTick]:
        """
        Ana veri yÃ¼kleme fonksiyonu.
        
        Args:
            symbol: Sembol adÄ± (Ã¶rn: "AAPL")
            use_cache: Cache kullanÄ±lsÄ±n mÄ±?
            start_date: BaÅŸlangÄ±Ã§ tarihi (filtreleme iÃ§in)
            end_date: BitiÅŸ tarihi (filtreleme iÃ§in)
        
        Returns:
            List[MarketTick]: YÃ¼klenmiÅŸ piyasa verileri
        """
        # Cache kontrolÃ¼
        if use_cache and symbol in self._cache:
            log.debug(f"ğŸ’¾ Cache'den yÃ¼kleniyor: {symbol}")
            self.stats['cache_hits'] += 1
            return self._apply_date_filter(self._cache[symbol], start_date, end_date)
        
        # Dosya yolunu bul
        file_path = self._find_data_file(symbol)
        
        if not file_path:
            log.error(f"âŒ Veri dosyasÄ± bulunamadÄ±: {symbol}")
            log.warning(f"ğŸ“ Aranan konum: {self.storage_path}")
            log.warning(f"ğŸ“ Beklenen format: {symbol}_15min.csv veya {symbol}_15min.xlsx")
            self.stats['errors'] += 1
            return []
        
        log.info(f"ğŸ“‚ Veri yÃ¼kleniyor: {file_path.name} ...")
        
        try:
            # Dosya formatÄ±na gÃ¶re yÃ¼kle
            if file_path.suffix.lower() == '.csv':
                df = self._load_csv(file_path)
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                df = self._load_excel(file_path)
            else:
                log.error(f"âŒ Desteklenmeyen dosya formatÄ±: {file_path.suffix}")
                return []
            
            if df is None or df.empty:
                log.error(f"âŒ Dosya okunamadÄ± veya boÅŸ: {file_path.name}")
                return []
            
            # Veri iÅŸleme pipeline
            df = self._preprocess_dataframe(df, symbol)
            
            if df.empty:
                log.error(f"âŒ Ã–n iÅŸleme sonrasÄ± veri boÅŸ: {symbol}")
                return []
            
            # Veri kalitesi kontrolÃ¼
            if self.validate_data:
                quality_report = self._validate_data_quality(df, symbol)
                self.quality_reports[symbol] = quality_report
                self._print_quality_report(quality_report)
            
            # MarketTick listesine dÃ¶nÃ¼ÅŸtÃ¼r
            ticks = self._dataframe_to_ticks(df, symbol)
            
            # Cache'e kaydet
            self._cache[symbol] = ticks
            
            # Ä°statistikleri gÃ¼ncelle
            self.stats['files_loaded'] += 1
            self.stats['total_rows'] += len(ticks)
            
            log.success(f"âœ… BAÅARILI: {len(ticks):,} adet mum verisi yÃ¼klendi ({file_path.name})")
            log.info(f"ğŸ“… Tarih aralÄ±ÄŸÄ±: {df.index[0]} â†’ {df.index[-1]}")
            
            return self._apply_date_filter(ticks, start_date, end_date)
            
        except Exception as e:
            log.critical(f"ğŸ’¥ HATA: CSV okuma hatasÄ± ({symbol}): {e}")
            log.exception(e)  # Full traceback
            self.stats['errors'] += 1
            return []
    
    def _find_data_file(self, symbol: str) -> Optional[Path]:
        """Sembol iÃ§in veri dosyasÄ±nÄ± bulur (CSV veya Excel)"""
        possible_files = [
            self.storage_path / f"{symbol}_15min.csv",
            self.storage_path / f"{symbol}_15min.xlsx",
            self.storage_path / f"{symbol}.csv",
            self.storage_path / f"{symbol}.xlsx",
            self.storage_path / f"{symbol.upper()}_15min.csv",
            self.storage_path / f"{symbol.lower()}_15min.csv",
        ]
        
        for file_path in possible_files:
            if file_path.exists():
                return file_path
        
        return None
    
    def _load_csv(self, file_path: Path) -> Optional[pd.DataFrame]:
        """CSV dosyasÄ±nÄ± yÃ¼kler"""
        try:
            # Otomatik delimiter detection
            with open(file_path, 'r') as f:
                first_line = f.readline()
                delimiter = ',' if ',' in first_line else ';'
            
            df = pd.read_csv(
                file_path,
                delimiter=delimiter,
                parse_dates=['timestamp'],
                na_values=['', 'NA', 'N/A', 'null', 'NULL']
            )
            return df
            
        except Exception as e:
            log.error(f"CSV okuma hatasÄ±: {e}")
            return None
    
    def _load_excel(self, file_path: Path) -> Optional[pd.DataFrame]:
        """Excel dosyasÄ±nÄ± yÃ¼kler"""
        try:
            df = pd.read_excel(
                file_path,
                parse_dates=['timestamp'],
                na_values=['', 'NA', 'N/A', 'null', 'NULL']
            )
            return df
            
        except Exception as e:
            log.error(f"Excel okuma hatasÄ±: {e}")
            return None
    
    def _preprocess_dataframe(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        Veri Ã¶n iÅŸleme pipeline:
        1. Kolon isimleri dÃ¼zelt
        2. Veri tiplerini dÃ¼zelt
        3. Timestamp'i index yap
        4. SÄ±ralama
        5. DuplikasyonlarÄ± temizle
        6. Eksik deÄŸerleri iÅŸle
        7. AykÄ±rÄ± deÄŸerleri temizle
        """
        # 1. Kolon isimleri standardize et
        df.columns = df.columns.str.lower().str.strip()
        
        # 2. Gerekli kolonlarÄ± kontrol et
        required_cols = ['timestamp', 'close']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            log.error(f"âŒ Eksik kolonlar: {missing_cols}")
            log.info(f"ğŸ“‹ Mevcut kolonlar: {list(df.columns)}")
            return pd.DataFrame()
        
        # 3. OHLC kolonlarÄ±nÄ± ekle (yoksa)
        if 'open' not in df.columns:
            df['open'] = df['close']
        if 'high' not in df.columns:
            df['high'] = df['close']
        if 'low' not in df.columns:
            df['low'] = df['close']
        if 'volume' not in df.columns:
            df['volume'] = 0
        
        # 4. Veri tiplerini dÃ¼zelt
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # 5. Timestamp'i datetime'a Ã§evir ve index yap
        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        
        df.set_index('timestamp', inplace=True)
        df.sort_index(inplace=True)
        
        # 6. DuplikasyonlarÄ± temizle
        duplicates = df.index.duplicated().sum()
        if duplicates > 0:
            log.warning(f"âš ï¸ {duplicates} duplikat zaman damgasÄ± kaldÄ±rÄ±ldÄ±")
            df = df[~df.index.duplicated(keep='first')]
        
        # 7. NaN/Inf deÄŸerleri temizle
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        
        # 8. Eksik deÄŸerleri interpolate et
        if self.interpolate_missing:
            missing_count = df.isnull().sum().sum()
            if missing_count > 0:
                log.warning(f"âš ï¸ {missing_count} eksik deÄŸer interpolate ediliyor...")
                df.interpolate(method='linear', inplace=True, limit_direction='both')
                df.fillna(method='ffill', inplace=True)  # Kalan NaN'larÄ± forward fill
                df.fillna(method='bfill', inplace=True)  # Hala NaN varsa backward fill
        
        # 9. AykÄ±rÄ± deÄŸerleri temizle
        if self.remove_outliers:
            df = self._remove_outliers(df)
        
        # 10. OHLC mantÄ±k kontrolÃ¼ (High >= Low, Close between them)
        df = self._fix_ohlc_logic(df)
        
        return df
    
    def _remove_outliers(self, df: pd.DataFrame, n_std: float = 5.0) -> pd.DataFrame:
        """
        Ä°statistiksel aykÄ±rÄ± deÄŸerleri kaldÄ±rÄ±r (Z-score method).
        Fiyatta %100+ deÄŸiÅŸim varsa suspicious olarak iÅŸaretle.
        """
        returns = df['close'].pct_change()
        
        # Z-score hesapla
        z_scores = np.abs((returns - returns.mean()) / returns.std())
        
        # AykÄ±rÄ± deÄŸerleri bul
        outliers = z_scores > n_std
        outlier_count = outliers.sum()
        
        if outlier_count > 0:
            log.warning(f"âš ï¸ {outlier_count} aykÄ±rÄ± deÄŸer tespit edildi ve dÃ¼zeltiliyor...")
            
            # AykÄ±rÄ± deÄŸerleri Ã¶nceki/sonraki deÄŸerlerin ortalamasÄ± ile deÄŸiÅŸtir
            df.loc[outliers, 'close'] = df['close'].rolling(window=3, center=True).mean()[outliers]
            df.loc[outliers, 'open'] = df['open'].rolling(window=3, center=True).mean()[outliers]
            df.loc[outliers, 'high'] = df['high'].rolling(window=3, center=True).mean()[outliers]
            df.loc[outliers, 'low'] = df['low'].rolling(window=3, center=True).mean()[outliers]
        
        return df
    
    def _fix_ohlc_logic(self, df: pd.DataFrame) -> pd.DataFrame:
        """OHLC mantÄ±k hatalarÄ±nÄ± dÃ¼zeltir"""
        # High en az close/open kadar olmalÄ±
        df['high'] = df[['high', 'close', 'open']].max(axis=1)
        
        # Low en fazla close/open kadar olmalÄ±
        df['low'] = df[['low', 'close', 'open']].min(axis=1)
        
        return df
    
    def _validate_data_quality(self, df: pd.DataFrame, symbol: str) -> DataQualityReport:
        """
        Veri kalitesini deÄŸerlendirir ve rapor oluÅŸturur.
        """
        # Eksik deÄŸerler
        missing = df.isnull().sum().to_dict()
        
        # Tarih aralÄ±ÄŸÄ±
        date_range = (df.index.min(), df.index.max())
        
        # Zaman boÅŸluklarÄ± (15 dakikalÄ±k barlar bekleniyor)
        expected_interval = timedelta(minutes=15)
        gaps = 0
        
        time_diffs = df.index.to_series().diff()
        large_gaps = time_diffs > expected_interval * 2  # 30 dakikadan fazla boÅŸluk
        gaps = large_gaps.sum()
        
        # Anomali tespiti (fiyat sÄ±Ã§ramalarÄ±)
        returns = df['close'].pct_change().abs()
        anomalies = (returns > 0.1).sum()  # %10'dan fazla deÄŸiÅŸim
        
        # Kalite skoru hesapla (0-100)
        quality_score = 100.0
        quality_score -= min(50, (sum(missing.values()) / len(df)) * 100)  # Eksik veri cezasÄ±
        quality_score -= min(20, (gaps / len(df)) * 1000)  # Gap cezasÄ±
        quality_score -= min(20, (anomalies / len(df)) * 100)  # Anomali cezasÄ±
        quality_score = max(0, quality_score)
        
        return DataQualityReport(
            symbol=symbol,
            total_rows=len(df),
            missing_values=missing,
            duplicates=0,  # Zaten temizlendi
            date_range=date_range,
            gaps_detected=gaps,
            anomalies=anomalies,
            quality_score=quality_score
        )
    
    def _print_quality_report(self, report: DataQualityReport):
        """Veri kalite raporunu yazdÄ±rÄ±r"""
        log.info("â”€" * 50)
        log.info(f"ğŸ“Š VERÄ° KALÄ°TESÄ° RAPORU: {report.symbol}")
        log.info("â”€" * 50)
        log.info(f"  Toplam SatÄ±r      : {report.total_rows:,}")
        log.info(f"  Tarih AralÄ±ÄŸÄ±     : {report.date_range[0]} â†’ {report.date_range[1]}")
        log.info(f"  Zaman BoÅŸluklarÄ±  : {report.gaps_detected}")
        log.info(f"  Anomali SayÄ±sÄ±    : {report.anomalies}")
        
        # Kalite skoru renkli gÃ¶ster
        score = report.quality_score
        if score >= 90:
            log.success(f"  âœ… KALÄ°TE SKORU   : {score:.1f}/100 (MÃ¼kemmel)")
        elif score >= 70:
            log.info(f"  âš ï¸  KALÄ°TE SKORU   : {score:.1f}/100 (Ä°yi)")
        else:
            log.warning(f"  âŒ KALÄ°TE SKORU   : {score:.1f}/100 (DÃ¼ÅŸÃ¼k - Dikkat!)")
        
        log.info("â”€" * 50)
    
    def _dataframe_to_ticks(self, df: pd.DataFrame, symbol: str) -> List[MarketTick]:
        """DataFrame'i MarketTick listesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"""
        ticks = []
        
        for timestamp, row in df.iterrows():
            tick = MarketTick(
                symbol=symbol,
                price=float(row['close']),
                volume=float(row['volume']),
                timestamp=timestamp,
                source="CSV_HISTORICAL"
            )
            ticks.append(tick)
        
        return ticks
    
    def _apply_date_filter(
        self, 
        ticks: List[MarketTick],
        start_date: Optional[datetime],
        end_date: Optional[datetime]
    ) -> List[MarketTick]:
        """Tarih filtresi uygular"""
        if not start_date and not end_date:
            return ticks
        
        filtered = []
        for tick in ticks:
            if start_date and tick.timestamp < start_date:
                continue
            if end_date and tick.timestamp > end_date:
                continue
            filtered.append(tick)
        
        if len(filtered) < len(ticks):
            log.info(f"ğŸ“… Tarih filtresi uygulandÄ±: {len(ticks)} â†’ {len(filtered)} bar")
        
        return filtered
    
    def load_multiple_symbols(
        self, 
        symbols: List[str],
        parallel: bool = True
    ) -> Dict[str, List[MarketTick]]:
        """
        Birden fazla sembolÃ¼ yÃ¼kler.
        
        Args:
            symbols: YÃ¼klenecek sembol listesi
            parallel: Paralel yÃ¼kleme (daha hÄ±zlÄ± ama RAM kullanÄ±r)
        
        Returns:
            Dict: {symbol: [ticks]}
        """
        log.info(f"ğŸ“š Toplu veri yÃ¼kleme baÅŸlÄ±yor: {len(symbols)} sembol...")
        
        results = {}
        
        for i, symbol in enumerate(symbols, 1):
            log.info(f"[{i}/{len(symbols)}] {symbol} yÃ¼kleniyor...")
            ticks = self.load_data(symbol, use_cache=True)
            if ticks:
                results[symbol] = ticks
        
        log.success(f"âœ… {len(results)}/{len(symbols)} sembol baÅŸarÄ±yla yÃ¼klendi")
        return results
    
    def get_statistics(self) -> Dict:
        """YÃ¼kleyici istatistiklerini dÃ¶ner"""
        return {
            **self.stats,
            'cache_size': len(self._cache),
            'quality_reports': len(self.quality_reports)
        }
    
    def clear_cache(self):
        """Cache'i temizler"""
        self._cache.clear()
        log.info("ğŸ—‘ï¸ Cache temizlendi")


# KULLANIM Ã–RNEÄÄ°
if __name__ == "__main__":
    loader = LocalCSVLoader(
        storage_path="data/storage",
        validate_data=True,
        interpolate_missing=True,
        remove_outliers=True
    )
    
    # Tek sembol yÃ¼kle
    ticks = loader.load_data("AAPL")
    
    # Ã‡oklu sembol yÃ¼kle
    # symbols = ["AAPL", "MSFT", "GOOGL"]
    # all_data = loader.load_multiple_symbols(symbols)
    
    # Ä°statistikleri gÃ¶ster
    # print(loader.get_statistics())
# =============================================================================
# AlphaTrade System - Machine Learning Configuration
# =============================================================================
# Institutional-grade ML configuration for quantitative trading
# All parameters externalized for production flexibility
# =============================================================================

# -----------------------------------------------------------------------------
# Random Seed Configuration
# -----------------------------------------------------------------------------
random_seeds:
  primary: 42              # Primary seed for reproducibility
  ensemble_seeds:          # Seeds for ensemble model diversity
    - 42
    - 123
    - 456
    - 789
    - 2024

# -----------------------------------------------------------------------------
# Periods Per Year for Different Timeframes
# -----------------------------------------------------------------------------
# Critical for annualization of returns and risk metrics
periods_per_year:
  tick: 16380000          # ~65,000 ticks/day * 252 days
  second_1: 23400         # 6.5 hours * 3600 seconds * 252 days / 252
  minute_1: 98280         # 390 minutes/day * 252 days
  minute_5: 19656         # 78 bars/day * 252 days
  minute_15: 6552         # 26 bars/day * 252 days
  minute_30: 3276         # 13 bars/day * 252 days
  hour_1: 1638            # 6.5 bars/day * 252 days
  hour_4: 410             # ~1.6 bars/day * 252 days
  daily: 252              # Trading days per year
  weekly: 52              # Weeks per year
  monthly: 12             # Months per year

# Default timeframe for the system
default_timeframe: minute_15

# -----------------------------------------------------------------------------
# Feature Selection Configuration
# -----------------------------------------------------------------------------
feature_selection:
  max_features: 50                  # Maximum features after selection
  min_features: 10                  # Minimum features to keep
  method: importance                # importance, variance, correlation, rfe
  importance_threshold: 0.001       # Minimum importance score
  variance_threshold: 0.01          # For variance-based selection
  correlation_threshold: 0.95       # Drop highly correlated features

# -----------------------------------------------------------------------------
# Cross-Validation Configuration
# -----------------------------------------------------------------------------
# CRITICAL: Proper purge gap prevents data leakage from lookback-based features
# Formula: purge_gap >= prediction_horizon + max_feature_lookback + buffer
cross_validation:
  n_splits: 5                       # Number of CV folds
  purge_gap: "auto"                 # "auto" calculates: horizon + lookback + buffer
  embargo_pct: 0.01                 # % of test set as embargo
  prediction_horizon: 5             # Bars ahead for prediction (15min * 5 = 75min)
  max_feature_lookback: 200         # Longest lookback in features (MA200, etc.)

  # Walk-forward validation
  walk_forward:
    train_period_bars: 5040         # ~2 months at 15-min
    test_period_bars: 1260          # ~1 month at 15-min
    step_size: null                 # null = test_period_bars
    expanding: false                # true = expanding window
    min_train_samples: 1000         # Minimum samples for training

# -----------------------------------------------------------------------------
# Hyperparameter Optimization (Optuna)
# -----------------------------------------------------------------------------
optimization:
  n_trials: 100                     # Number of optimization trials
  timeout: 3600                     # Maximum time in seconds (1 hour)
  n_jobs: -1                        # Parallel jobs (-1 = all cores)
  direction: maximize               # maximize or minimize
  objective_metric: sharpe_ratio    # Primary optimization metric

  # Sampler configuration
  sampler:
    type: TPESampler                # TPESampler, CmaEsSampler, etc.
    seed: 42
    n_startup_trials: 10

  # Pruner configuration
  pruner:
    type: MedianPruner              # MedianPruner, HyperbandPruner
    n_startup_trials: 5
    n_warmup_steps: 10
    interval_steps: 1

  # Multi-objective optimization
  multi_objective:
    enabled: false
    objectives:
      - sharpe_ratio
      - max_drawdown
    directions:
      - maximize
      - minimize

# -----------------------------------------------------------------------------
# Model-Specific Configurations
# -----------------------------------------------------------------------------
models:
  # LightGBM Configuration
  lightgbm:
    default_params:
      boosting_type: gbdt
      objective: binary               # binary, multiclass, regression
      metric: auc
      n_estimators: 100
      max_depth: -1
      num_leaves: 31
      learning_rate: 0.1
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.1
      reg_lambda: 0.1
      min_child_samples: 20
      verbose: -1
      n_jobs: -1

    # Hyperparameter search space for Optuna
    param_space:
      n_estimators:
        type: int
        low: 50
        high: 500
      max_depth:
        type: int
        low: 3
        high: 12
      num_leaves:
        type: int
        low: 20
        high: 100
      learning_rate:
        type: float_log
        low: 0.01
        high: 0.3
      subsample:
        type: float
        low: 0.6
        high: 1.0
      colsample_bytree:
        type: float
        low: 0.6
        high: 1.0
      reg_alpha:
        type: float_log
        low: 1.0e-8
        high: 10.0
      reg_lambda:
        type: float_log
        low: 1.0e-8
        high: 10.0
      min_child_samples:
        type: int
        low: 10
        high: 100

  # XGBoost Configuration
  xgboost:
    default_params:
      objective: binary:logistic
      eval_metric: auc
      n_estimators: 100
      max_depth: 6
      learning_rate: 0.1
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.1
      reg_lambda: 0.1
      min_child_weight: 1
      gamma: 0
      tree_method: hist
      verbosity: 0
      n_jobs: -1

    param_space:
      n_estimators:
        type: int
        low: 50
        high: 500
      max_depth:
        type: int
        low: 3
        high: 12
      learning_rate:
        type: float_log
        low: 0.01
        high: 0.3
      subsample:
        type: float
        low: 0.6
        high: 1.0
      colsample_bytree:
        type: float
        low: 0.6
        high: 1.0
      reg_alpha:
        type: float_log
        low: 1.0e-8
        high: 10.0
      reg_lambda:
        type: float_log
        low: 1.0e-8
        high: 10.0
      min_child_weight:
        type: int
        low: 1
        high: 20
      gamma:
        type: float
        low: 0
        high: 5

  # CatBoost Configuration
  catboost:
    default_params:
      objective: Logloss
      eval_metric: AUC
      iterations: 100
      depth: 6
      learning_rate: 0.1
      l2_leaf_reg: 3
      bootstrap_type: Bayesian
      bagging_temperature: 1
      random_strength: 1
      verbose: false
      thread_count: -1

    param_space:
      iterations:
        type: int
        low: 50
        high: 500
      depth:
        type: int
        low: 3
        high: 10
      learning_rate:
        type: float_log
        low: 0.01
        high: 0.3
      l2_leaf_reg:
        type: float_log
        low: 1.0
        high: 10.0
      bagging_temperature:
        type: float
        low: 0
        high: 3
      random_strength:
        type: float
        low: 0
        high: 3

  # Random Forest Configuration
  random_forest:
    default_params:
      n_estimators: 100
      max_depth: null
      min_samples_split: 2
      min_samples_leaf: 1
      max_features: sqrt
      bootstrap: true
      n_jobs: -1

    param_space:
      n_estimators:
        type: int
        low: 50
        high: 300
      max_depth:
        type: int
        low: 5
        high: 30
      min_samples_split:
        type: int
        low: 2
        high: 20
      min_samples_leaf:
        type: int
        low: 1
        high: 10
      max_features:
        type: categorical
        choices:
          - sqrt
          - log2
          - 0.5
          - 0.7

# -----------------------------------------------------------------------------
# Deep Learning Configuration
# -----------------------------------------------------------------------------
deep_learning:
  # General settings
  device: auto                      # auto, cpu, cuda, mps
  mixed_precision: true             # Use FP16 for faster training
  gradient_clip: 1.0                # Max gradient norm

  # Training defaults
  training:
    batch_size: 64
    max_epochs: 100
    patience: 10                    # Early stopping patience
    min_delta: 1.0e-4               # Min improvement for early stopping

  # Learning rate scheduling
  lr_scheduler:
    type: OneCycleLR                # OneCycleLR, ReduceLROnPlateau, CosineAnnealingLR
    max_lr: 1.0e-3
    pct_start: 0.3
    div_factor: 25.0
    final_div_factor: 10000.0

  # LSTM Configuration
  lstm:
    default_params:
      input_size: null              # Set from data
      hidden_size: 128
      num_layers: 2
      dropout: 0.2
      bidirectional: false
      batch_first: true

    param_space:
      hidden_size:
        type: categorical
        choices: [64, 128, 256, 512]
      num_layers:
        type: int
        low: 1
        high: 4
      dropout:
        type: float
        low: 0.1
        high: 0.5
      bidirectional:
        type: categorical
        choices: [true, false]

  # Transformer Configuration
  transformer:
    default_params:
      d_model: 128
      nhead: 4
      num_encoder_layers: 2
      num_decoder_layers: 2
      dim_feedforward: 512
      dropout: 0.1

    param_space:
      d_model:
        type: categorical
        choices: [64, 128, 256]
      nhead:
        type: categorical
        choices: [2, 4, 8]
      num_encoder_layers:
        type: int
        low: 1
        high: 4
      dim_feedforward:
        type: categorical
        choices: [256, 512, 1024]
      dropout:
        type: float
        low: 0.1
        high: 0.5

  # Temporal Fusion Transformer Configuration
  tft:
    default_params:
      hidden_size: 160
      attention_head_size: 4
      num_attention_heads: 4
      hidden_continuous_size: 16
      dropout: 0.1
      output_size: 1

# -----------------------------------------------------------------------------
# Custom Loss Functions
# -----------------------------------------------------------------------------
loss_functions:
  # Sharpe ratio loss
  sharpe:
    enabled: true
    risk_free_rate: 0.0             # Adjust for current rates

  # Sortino ratio loss
  sortino:
    enabled: true
    target_return: 0.0

  # Max drawdown penalty
  max_drawdown:
    enabled: true
    penalty_weight: 1.0

  # Combined multi-objective loss
  combined:
    sharpe_weight: 0.5
    sortino_weight: 0.3
    drawdown_weight: 0.2

# -----------------------------------------------------------------------------
# MLflow Configuration
# -----------------------------------------------------------------------------
mlflow:
  tracking_uri: mlruns              # Local or remote URI
  experiment_prefix: alphatrade
  artifact_location: null           # null = use default

  # Auto-logging settings
  autolog:
    sklearn: true
    lightgbm: true
    xgboost: true
    pytorch: false                  # Can cause issues, enable carefully

  # Tags to add to all runs
  default_tags:
    project: alphatrade
    environment: development        # development, staging, production

# -----------------------------------------------------------------------------
# Model Registry Configuration
# -----------------------------------------------------------------------------
model_registry:
  # Staging workflow
  stages:
    - None
    - Staging
    - Production
    - Archived

  # Auto-promotion rules
  auto_promote:
    enabled: false
    staging_threshold:              # Metrics required for staging
      sharpe_ratio: 0.5
      max_drawdown: 0.30
    production_threshold:           # Metrics required for production
      sharpe_ratio: 1.0
      max_drawdown: 0.20
      min_observations: 252         # At least 1 year of data

# -----------------------------------------------------------------------------
# Market Regime Detection Configuration (Directive 5.1)
# -----------------------------------------------------------------------------
regime_detection:
  # Detection method: hmm, volatility, trend, composite
  method: composite

  # HMM regime detector
  hmm:
    n_regimes: 3                    # Number of hidden states
    n_iter: 100                     # EM iterations
    covariance_type: full           # full, diag, tied, spherical
    features:                       # Features for regime detection
      - returns
      - volatility
      - volume_ratio
    lookback_period: 252            # Days for regime calibration

  # Volatility-based regime detector
  volatility:
    method: percentile              # percentile, rolling_std, garch
    lookback: 60                    # Rolling window for volatility
    low_threshold: 0.33             # Percentile cutoff for low vol regime
    high_threshold: 0.67            # Percentile cutoff for high vol regime

  # Trend regime detector (ADX-based)
  trend:
    adx_period: 14                  # ADX calculation period
    trending_threshold: 25          # ADX above = trending market
    strong_trend_threshold: 40      # ADX above = strong trend

  # Per-regime position scaling
  regime_position_scaling:
    low_volatility:
      position_multiplier: 1.2      # Scale up in calm markets
      max_position_pct: 0.15
      prediction_horizon: 5
    normal:
      position_multiplier: 1.0
      max_position_pct: 0.10
      prediction_horizon: 5
    high_volatility:
      position_multiplier: 0.5      # Scale down in volatile markets
      max_position_pct: 0.05
      prediction_horizon: 3         # Shorter horizon in volatile markets

  # Per-regime model parameters (optional overrides)
  regime_model_params:
    low_volatility:
      lightgbm:
        n_estimators: 150
        learning_rate: 0.05
    high_volatility:
      lightgbm:
        n_estimators: 50
        learning_rate: 0.15
        max_depth: 4                # Simpler model for volatile regimes

# -----------------------------------------------------------------------------
# Online Learning Configuration (Directive 5.2)
# -----------------------------------------------------------------------------
online_learning:
  enabled: false                    # Enable incremental model updates

  # Update frequency
  update_frequency: weekly          # daily, weekly, on_drift
  update_schedule:
    day_of_week: 0                  # Monday (0) for weekly updates
    hour: 2                         # 2 AM local time

  # Minimum samples for update
  min_samples_for_update: 500
  max_model_age_days: 30            # Force retrain after N days

  # Drift-triggered updates
  drift_detection:
    enabled: true
    method: adwin                   # adwin, ddm, eddm, page_hinkley
    adwin_delta: 0.002              # ADWIN sensitivity
    ddm_warning_level: 2.0          # DDM warning threshold
    ddm_out_control_level: 3.0      # DDM drift threshold
    page_hinkley_delta: 0.005       # Page-Hinkley threshold
    page_hinkley_lambda: 50         # Page-Hinkley threshold

  # Incremental training settings
  incremental:
    warm_start: true                # Continue from existing model
    learning_rate_decay: 0.95       # LR decay per update
    memory_limit_mb: 500            # Max memory for sample replay
    importance_weighting: true      # Weight recent samples higher
    sample_decay: 0.99              # Weight decay for old samples

  # Model versioning
  versioning:
    keep_n_versions: 5              # Number of model versions to keep
    rollback_on_degradation: true   # Auto-rollback if performance drops
    performance_threshold: 0.9      # Min relative performance vs previous

# -----------------------------------------------------------------------------
# Feature Store Configuration (Directive 5.3)
# -----------------------------------------------------------------------------
feature_store:
  # Backend selection: redis, timescaledb, parquet, sqlite
  backend: parquet

  # Parquet backend settings
  parquet:
    base_path: data/feature_store
    partition_by: symbol            # Partition by symbol for efficiency
    compression: snappy             # snappy, gzip, lz4
    row_group_size: 100000

  # Redis backend settings (for real-time)
  redis:
    host: localhost
    port: 6379
    db: 0
    key_prefix: alphatrade:features
    ttl_seconds: 86400              # 24 hours

  # TimescaleDB backend settings
  timescaledb:
    host: localhost
    port: 5432
    database: alphatrade_features
    schema: feature_store
    hypertable_chunk_interval: 7d   # Chunk by week

  # Cache settings
  cache:
    enabled: true
    ttl_seconds: 3600               # 1 hour cache TTL
    max_size_mb: 1000               # Max cache size in memory
    eviction_policy: lru            # lru, lfu, ttl

  # Feature versioning
  versioning:
    enabled: true
    track_lineage: true             # Track feature computation lineage
    retention_days: 90              # Keep feature versions for 90 days

  # Monitoring and alerting
  monitoring:
    stale_feature_threshold_hours: 2  # Alert if feature older than N hours
    missing_feature_alert: true
    null_ratio_threshold: 0.05      # Alert if >5% null values

# -----------------------------------------------------------------------------
# Model Monitoring Configuration (Directive 5.4)
# -----------------------------------------------------------------------------
monitoring:
  enabled: true

  # Performance monitoring thresholds
  performance:
    sharpe_ratio_warning: 0.5       # Warn if Sharpe drops below
    sharpe_ratio_critical: 0.0      # Critical if Sharpe goes negative
    max_drawdown_warning: 0.15      # Warn at 15% drawdown
    max_drawdown_critical: 0.25     # Critical at 25% drawdown
    win_rate_warning: 0.45          # Warn if win rate drops below 45%
    win_rate_critical: 0.40         # Critical below 40%
    profit_factor_warning: 1.0      # Warn if profit factor < 1
    ic_degradation_warning: 0.3     # Warn if IC drops 30% from baseline

  # Feature drift alert thresholds (PSI)
  drift:
    psi_warning: 0.10               # PSI warning threshold
    psi_critical: 0.25              # PSI critical threshold
    ks_pvalue_warning: 0.05         # KS test warning p-value
    check_frequency: daily          # daily, hourly, per_prediction
    features_to_monitor: null       # null = all features

  # Prediction latency thresholds
  latency:
    warning_ms: 50                  # Warn if prediction > 50ms
    critical_ms: 100                # Critical if prediction > 100ms
    track_percentiles:              # Track these percentiles
      - 50
      - 95
      - 99
    batch_size_for_test: 100        # Samples for latency testing

  # Alerting configuration
  alerting:
    enabled: true

    # Email alerts
    email:
      enabled: false
      smtp_host: smtp.gmail.com
      smtp_port: 587
      from_address: alerts@alphatrade.com
      to_addresses:
        - team@alphatrade.com
      subject_prefix: "[AlphaTrade Alert]"

    # Slack alerts
    slack:
      enabled: false
      webhook_url: null             # Slack webhook URL
      channel: "#ml-alerts"
      mention_on_critical: "@here"

    # PagerDuty integration
    pagerduty:
      enabled: false
      routing_key: null             # PagerDuty routing key
      severity_mapping:
        warning: warning
        critical: critical

  # Monitoring report generation
  reports:
    daily_report: true
    weekly_summary: true
    report_output_path: reports/monitoring
    include_plots: true
    retention_days: 30

# -----------------------------------------------------------------------------
# Training Pipeline Configuration
# -----------------------------------------------------------------------------
training:
  # Default model type
  model_type: lightgbm_classifier

  # Train/test split
  train_ratio: 0.8
  test_size: 0.2

  # Cross-validation (uses combinatorial purged by default)
  cv_type: combinatorial_purged     # combinatorial_purged, purged, walk_forward, group_time_series
  cv_splits: 5
  n_test_splits: 2                  # For combinatorial CV
  embargo_pct: 0.01

  # Walk-forward specific
  wf_train_period: 252              # Trading days for WF training window
  wf_test_period: 21                # Trading days for WF test window

  # Drift detection thresholds
  drift_psi_threshold: 0.2          # PSI threshold for drift detection stage
  adversarial_warn_auc: 0.55        # Adversarial validation warning threshold
  adversarial_critical_auc: 0.60    # Adversarial validation critical threshold

  # Early stopping
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.001

# -----------------------------------------------------------------------------
# Fractional Differentiation Configuration
# -----------------------------------------------------------------------------
fractional_diff:
  enabled: true                     # Enable in feature pipeline
  d_range: [0.0, 1.0]               # Range for d parameter search
  threshold: 0.01                   # ADF test threshold for stationarity
  max_lag: null                     # null = auto-calculate
  columns:                          # Columns to fractionally differentiate
    - close
    - open
    - high
    - low
    - volume

# -----------------------------------------------------------------------------
# Meta-Labeling Configuration
# -----------------------------------------------------------------------------
meta_labeling:
  enabled: false                    # Enable meta-labeling for position sizing
  primary_model: lightgbm_classifier
  meta_model: lightgbm_classifier
  bet_size_from_proba: true         # Use probabilities for bet sizing
  min_probability: 0.55             # Min probability to take position
  max_bet_size: 1.0                 # Maximum bet size multiplier

# -----------------------------------------------------------------------------
# Triple-Barrier Labeling Configuration
# -----------------------------------------------------------------------------
triple_barrier:
  enabled: false                    # Use triple-barrier instead of simple returns
  take_profit_bps: 50               # Take profit in basis points
  stop_loss_bps: 25                 # Stop loss in basis points
  time_barrier_bars: 20             # Maximum holding period
  min_return_bps: 5                 # Minimum return to label as non-zero

# -----------------------------------------------------------------------------
# Position Sizing Configuration
# -----------------------------------------------------------------------------
position_sizing:
  method: kelly                     # kelly, fixed, volatility_scaled, confidence_weighted
  kelly:
    fraction: 0.25                  # Kelly fraction (conservative = 0.25)
    max_position: 0.10              # Max position as % of capital
    min_position: 0.01              # Min position as % of capital
  volatility_scaled:
    target_volatility: 0.15         # Target annualized volatility
    lookback: 20                    # Volatility lookback period
  confidence_weighted:
    use_meta_labels: true           # Use meta-model confidence
    confidence_threshold: 0.6       # Min confidence to trade
